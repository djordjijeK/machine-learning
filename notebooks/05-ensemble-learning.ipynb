{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c67639f",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "\n",
    "In broad terms, ensemble methods combine multiple individual models into an ensemble such that the ensemble has a better performance than an individual model on average.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e065db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc7572d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_CLASSIFICATION = '../data/iris.csv'\n",
    "PATH_REGRESSION     = '../data/rent.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93a097b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength[cm]</th>\n",
       "      <th>SepalWidth[cm]</th>\n",
       "      <th>PetalLength[cm]</th>\n",
       "      <th>PetalWidth[cm]</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLength[cm]  SepalWidth[cm]  PetalLength[cm]  PetalWidth[cm]  Species\n",
       "0              5.1             3.5              1.4             0.2        0\n",
       "1              4.9             3.0              1.4             0.2        0\n",
       "2              4.7             3.2              1.3             0.2        0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_mapping = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\n",
    "\n",
    "data_classification = pd.read_csv(filepath_or_buffer = PATH_CLASSIFICATION)\n",
    "data_classification['Species'] = data_classification['Species'].map(label_mapping)\n",
    "data_classification = data_classification.drop('Id', axis = 1)\n",
    "data_classification.head(n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "347b06ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bedrooms</th>\n",
       "      <th>Bathrooms</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.7108</td>\n",
       "      <td>-73.9539</td>\n",
       "      <td>2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.7513</td>\n",
       "      <td>-73.9722</td>\n",
       "      <td>3800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.7575</td>\n",
       "      <td>-73.9625</td>\n",
       "      <td>3495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bedrooms  Bathrooms  Latitude  Longitude  Price\n",
       "0         1        1.0   40.7108   -73.9539   2400\n",
       "1         2        1.0   40.7513   -73.9722   3800\n",
       "2         2        1.0   40.7575   -73.9625   3495"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_regression = pd.read_csv(filepath_or_buffer = PATH_REGRESSION, names = ['Bedrooms', 'Bathrooms', 'Latitude', 'Longitude', 'Price'], skiprows = 1)\n",
    "data_regression.head(n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27ecc1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (112, 4), Classes shape: (112,)\n",
      "Validation set shape: (38, 4), Classes shape: (38,)\n",
      "Train set class distribution: [38 37 37]. Validation set class distribution: [12 13 13]\n"
     ]
    }
   ],
   "source": [
    "X_class_train, X_class_valid, y_class_train, y_class_valid = train_test_split(data_classification.drop('Species', axis = 1).values, data_classification['Species'].values, test_size = 0.25, random_state = 123, stratify = data_classification['Species'].values)\n",
    "\n",
    "print(f'Training set shape: {X_class_train.shape}, Classes shape: {y_class_train.shape}')\n",
    "print(f'Validation set shape: {X_class_valid.shape}, Classes shape: {y_class_valid.shape}')\n",
    "print(f'Train set class distribution: {np.bincount(y_class_train)}. Validation set class distribution: {np.bincount(y_class_valid)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e4b69df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (36225, 4), Classes shape: (36225,)\n",
      "Validation set shape: (12075, 4), Classes shape: (12075,)\n"
     ]
    }
   ],
   "source": [
    "X_regression_train, X_regression_valid, y_regression_train, y_regression_valid = train_test_split(data_regression.drop('Price', axis = 1).values, data_regression['Price'].values, test_size = 0.25, random_state = 123)\n",
    "\n",
    "print(f'Training set shape: {X_regression_train.shape}, Classes shape: {y_regression_train.shape}')\n",
    "print(f'Validation set shape: {X_regression_valid.shape}, Classes shape: {y_regression_valid.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903040aa",
   "metadata": {},
   "source": [
    "## A. Majority and Soft Majority Voting\n",
    "\n",
    "Let us assume that we have $n$ different models $\\{h_1, h_2, \\dots, h_n\\}$ where $h_i(\\vec{x}) = \\hat{y_i}$.\n",
    "In the case of classifiers, the final ensemble prediction is: \n",
    "\n",
    "$$\\hat{y}_{pred} = mode\\{h_1(\\vec{x}), h_2(\\vec{x}), ... h_n(\\vec{x})\\}$$\n",
    "\n",
    "In the case of regressors, the final ensemble prediction is: \n",
    "\n",
    "$$\\hat{y}_{pred} = \\frac{1}{n}\\sum_{i = 1}^{n} h_i(\\vec{x})$$\n",
    "\n",
    "\n",
    "In soft majority voting scheme, the class prediction is calculated as:\n",
    "\n",
    "$$ \\hat{y}_{pred} = \\arg\\!\\max_j \\sum_{i = 1}^{n} w_ip_{i,j} $$\n",
    "\n",
    "where $p_{i,j}$ is the predicted class membership probability for class label $j$ by the $i$th classifier.\n",
    "Here $w_i$ is an optional weighting parameter (possibly explaining the model reliability).\n",
    "To illustrate this, let us assume we have a binary classification problem with class labels\n",
    "$j \\in {0, 1}$ and an ensemble of three classifiers $h_i(i \\in {1, 2, 3})$:\n",
    "\n",
    "$$ h_1(x) = [0.9, 0.1], h_2(x) = [0.8, 0.2], h_3(x) = [0.4, 0.6] $$\n",
    "\n",
    "We can then calculate the individual class membership probabilities as follows:\n",
    "\n",
    "$$ p(j = 0 \\mid x) = 0.2*0.9 + 0.2*0.8 + 0.6*0.4 = 0.58 $$\n",
    "\n",
    "$$ p(j = 1 \\mid x) = 0.2*0.1+0.2*0.2+0.6*0.6 = 0.42 $$\n",
    "\n",
    "The predicted class label is then:\n",
    "\n",
    "$$ \\hat{y}_{pred} = \\arg\\!\\max_j \\{p(j = 0 \\mid x), p(j = 1 \\mid x)\\} = 0 $$\n",
    "\n",
    "In the case of regressors, the final ensemble prediction is:\n",
    "\n",
    "$$ \\hat{y}_{pred} =  \\frac{1}{\\sum_{i = 1}^{n}w_i}\\sum_{i = 1}^{n} w_ih_i({\\vec{x})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c97d5e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "151eb462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier models\n",
    "classifier1 = DecisionTreeClassifier(random_state = 1, max_depth = None)\n",
    "classifier2 = DecisionTreeClassifier(random_state = 1, max_depth = 1)\n",
    "classifier3 = DecisionTreeClassifier(random_state = 1, max_depth = 2)\n",
    "\n",
    "# ensemble of classifiers\n",
    "ensemble_hard = VotingClassifier(estimators = [('classifier1', classifier1), ('classifier2', classifier2), ('classifier3', classifier3)], weights = [1, 1, 1], voting = 'hard')\n",
    "ensemble_soft = VotingClassifier(estimators = [('classifier1', classifier1), ('classifier2', classifier2), ('classifier3', classifier3)], weights = [1, 1, 1], voting = 'soft')\n",
    "\n",
    "classifier1.fit(X = X_class_train, y = y_class_train);\n",
    "classifier2.fit(X = X_class_train, y = y_class_train);\n",
    "classifier3.fit(X = X_class_train, y = y_class_train);\n",
    "ensemble_hard.fit(X = X_class_train, y = y_class_train);\n",
    "ensemble_soft.fit(X = X_class_train, y = y_class_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17d924d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector: [5.8 2.7 5.1 1.9], Class: 2\n",
      "Classifier 1 = [2], Classifier 2 = [1], Classifier 3 = [2]\n",
      "Ensemble (hard) = [2]\n",
      "Accuracy -- classifier 1 = 0.9736842105263158\n",
      "Accuracy -- classifier 2 = 0.6578947368421053\n",
      "Accuracy -- classifier 3 = 0.9473684210526315\n",
      "Accuracy -- ensemble (hard) = 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "print(f'Feature vector: {X_class_valid[31]}, Class: {y_class_valid[31]}')\n",
    "print(f'Classifier 1 = {classifier1.predict([X_class_valid[31]])}, Classifier 2 = {classifier2.predict([X_class_valid[31]])}, Classifier 3 = {classifier3.predict([X_class_valid[31]])}')\n",
    "print(f'Ensemble (hard) = {ensemble_hard.predict([X_class_valid[31]])}')\n",
    "\n",
    "print(f'Accuracy -- classifier 1 = {classifier1.score(X_class_valid, y_class_valid)}')\n",
    "print(f'Accuracy -- classifier 2 = {classifier2.score(X_class_valid, y_class_valid)}')\n",
    "print(f'Accuracy -- classifier 3 = {classifier3.score(X_class_valid, y_class_valid)}')\n",
    "print(f'Accuracy -- ensemble (hard) = {ensemble_hard.score(X_class_valid, y_class_valid)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31dc6658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector: [5.8 2.7 5.1 1.9], Class: 2\n",
      "Classifier 1 = [0. 0. 1.], Classifier 2 = [0.  0.5 0.5], Classifier 3 = [0.  0.1 0.9]\n",
      "Ensemble (soft) = [0.  0.2 0.8]\n",
      "Accuracy -- classifier 1 = 0.9736842105263158\n",
      "Accuracy -- classifier 2 = 0.6578947368421053\n",
      "Accuracy -- classifier 3 = 0.9473684210526315\n",
      "Accuracy -- ensemble (hard) = 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "print(f'Feature vector: {X_class_valid[31]}, Class: {y_class_valid[31]}')\n",
    "print(f'Classifier 1 = {classifier1.predict_proba([X_class_valid[31]])[0]}, Classifier 2 = {classifier2.predict_proba([X_class_valid[31]])[0]}, Classifier 3 = {classifier3.predict_proba([X_class_valid[31]])[0]}')\n",
    "print(f'Ensemble (soft) = {ensemble_soft.predict_proba([X_class_valid[31]])[0]}')\n",
    "\n",
    "print(f'Accuracy -- classifier 1 = {classifier1.score(X_class_valid, y_class_valid)}')\n",
    "print(f'Accuracy -- classifier 2 = {classifier2.score(X_class_valid, y_class_valid)}')\n",
    "print(f'Accuracy -- classifier 3 = {classifier3.score(X_class_valid, y_class_valid)}')\n",
    "print(f'Accuracy -- ensemble (hard) = {ensemble_soft.score(X_class_valid, y_class_valid)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1ae7da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression models\n",
    "regressor1 = DecisionTreeRegressor(random_state = 1, max_depth = None)\n",
    "regressor2 = DecisionTreeRegressor(random_state = 1, max_depth = 1)\n",
    "regressor3 = DecisionTreeRegressor(random_state = 1, max_depth = 2)\n",
    "\n",
    "# ensemble of regressors\n",
    "ensemble_equal_weights = VotingRegressor(estimators = [('regressor1', regressor1), ('regressor2', regressor2), ('regressor3', regressor3)], weights = [1, 1, 1])\n",
    "ensemble_unequal_weights = VotingRegressor(estimators = [('regressor1', regressor1), ('regressor2', regressor2), ('regressor3', regressor3)], weights = [5, 2, 1])\n",
    "\n",
    "regressor1.fit(X = X_regression_train, y = y_regression_train);\n",
    "regressor2.fit(X = X_regression_train, y = y_regression_train);\n",
    "regressor3.fit(X = X_regression_train, y = y_regression_train);\n",
    "ensemble_equal_weights.fit(X = X_regression_train, y = y_regression_train);\n",
    "ensemble_unequal_weights.fit(X = X_regression_train, y = y_regression_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1a5c138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector: [  3.       2.      40.7773 -73.9532], Target: 3800\n",
      "Regressor 1 = [3800.], Regressor 2 = [5315.39510543], Regressor 3 = [5587.29809763]\n",
      "Ensemble (equal weights) = [4900.89773435]\n",
      "Ensemble (unequal weights) = [4402.26103856]\n",
      "R^2 score -- regressor 1 = 0.8169192047486176\n",
      "R^2 score -- regressor 2 = 0.3951159880255374\n",
      "R^2 score -- regressor 3 = 0.49252838519484055\n",
      "R^2 score -- ensemble (equal weights) = 0.6931598270591892\n",
      "R^2 score -- ensemble (unequal weights) = 0.797543170017368\n"
     ]
    }
   ],
   "source": [
    "print(f'Feature vector: {X_regression_valid[0]}, Target: {y_regression_valid[0]}')\n",
    "print(f'Regressor 1 = {regressor1.predict([X_regression_valid[0]])}, Regressor 2 = {regressor2.predict([X_regression_valid[0]])}, Regressor 3 = {regressor3.predict([X_regression_valid[0]])}')\n",
    "print(f'Ensemble (equal weights) = {ensemble_equal_weights.predict([X_regression_valid[0]])}')\n",
    "print(f'Ensemble (unequal weights) = {ensemble_unequal_weights.predict([X_regression_valid[0]])}')\n",
    "\n",
    "print(f'R^2 score -- regressor 1 = {regressor1.score(X_regression_valid, y_regression_valid)}')\n",
    "print(f'R^2 score -- regressor 2 = {regressor2.score(X_regression_valid, y_regression_valid)}')\n",
    "print(f'R^2 score -- regressor 3 = {regressor3.score(X_regression_valid, y_regression_valid)}')\n",
    "print(f'R^2 score -- ensemble (equal weights) = {ensemble_equal_weights.score(X_regression_valid, y_regression_valid)}')\n",
    "print(f'R^2 score -- ensemble (unequal weights) = {ensemble_unequal_weights.score(X_regression_valid, y_regression_valid)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed2917b",
   "metadata": {},
   "source": [
    "## B. Bagging\n",
    "\n",
    "Bagging relies on a concept similar to majority voting but uses the same learning algorithm (typically a decision tree algorithm) to fit models on different subsets of the training data (bootstrap samples).\n",
    "In a nutshell, a bootstrap sample is a sample of size $n$ drawn with replacement from an original training set $\\mathcal{D}$ with $|\\mathcal{D}| = n$. \n",
    "Consequently, some training examples are duplicated in each bootstrap sample, and some other training examples do not appear in a given bootstrap sample at all (usually, we refer to these examples as \"out-of-bag sample\".)\n",
    "In the limit, there are approx. $63.2\\%$ unique examples in a given bootstrap sample.\n",
    "Consequently, $37.2\\%$ examples from the original training set won’t appear in a given bootstrap sample at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "015bf111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecd60f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Accuracy: 0.94%\n",
      "Validation Accuracy: 0.97%\n"
     ]
    }
   ],
   "source": [
    "classification_tree = DecisionTreeClassifier(criterion = 'entropy', max_depth = None, random_state = 1)\n",
    "bagging_classifier = BaggingClassifier(base_estimator = classification_tree, n_estimators = 100, bootstrap_features = False, oob_score = True, n_jobs = -1, random_state = 123)\n",
    "\n",
    "bagging_classifier.fit(X = X_class_train, y = y_class_train)\n",
    "\n",
    "print(f'OOB Accuracy: {bagging_classifier.oob_score_:.2f}%')\n",
    "print(f'Validation Accuracy: {bagging_classifier.score(X_class_valid, y_class_valid):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d1a47ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Score: 0.86%\n",
      "Validation R^2 Score: 0.85%\n"
     ]
    }
   ],
   "source": [
    "regression_tree = DecisionTreeRegressor(max_depth = None, random_state = 1)\n",
    "bagging_regressor = BaggingRegressor(base_estimator = regression_tree, n_estimators = 100, bootstrap_features = False, oob_score = True, n_jobs = -1, random_state = 123)\n",
    "\n",
    "bagging_regressor.fit(X = X_regression_train, y = y_regression_train)\n",
    "\n",
    "print(f'OOB Score: {bagging_regressor.oob_score_:.2f}%')\n",
    "print(f'Validation R^2 Score: {bagging_regressor.score(X_regression_valid, y_regression_valid):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c4594b",
   "metadata": {},
   "source": [
    "## C. Boosting\n",
    "\n",
    "There are two broad categories of boosting: adaptive boosting and gradient boosting.\n",
    "Boosting is an iterative process, where the training set is reweighted, at each iteration, based on mistakes a previous leaner made (i.e., misclassifications); the two approaches, adaptive and gradient boosting, differ mainly regarding how the weights are updated and how the classifiers are combined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c343ad0",
   "metadata": {},
   "source": [
    "### C.1 Adaptive Boosting - AdaBoost\n",
    "\n",
    "Intuitively, we can outline the general boosting procedure for AdaBoost as follows:\n",
    "\n",
    "  - Initialize a weight vector with uniform weights.\n",
    "  \n",
    "  - Loop:\n",
    "  \n",
    "    - Apply *weak learner* to weighted training examples (instead of original training set, may draw bootstrap samples with weighted probability).\n",
    "    \n",
    "    - Increase weight for misclassified examples.\n",
    "    \n",
    "  - (Weighted) majority voting on trained classifiers.\n",
    "  \n",
    "**Algorithm**:\n",
    "\n",
    "  1. Initialize $k$: the number of AdaBoost rounds.\n",
    "  2. Initialize $\\mathcal{D}$: the training dataset, $\\mathcal{D} = {\\{ <\\vec{x_1}, y_1>, \\dots, <\\vec{x_n}, y_n> \\}}$\n",
    "  3. Initialize $w_1(i) = \\frac{1}{n}$, for $i = 1, \\dots, n$\n",
    "  4. **loop** $r = 1 \\dots k$:\n",
    "     - $w_r(i) = \\frac{w_r(i)}{\\sum_{i = 1}^{n} w_r(i)}$ for $i = 1, \\dots, n$\n",
    "     \n",
    "     - $h_r = FitWeakLearner(\\mathcal{D}, \\vec{w_r})$ -- [weighted dataset]\n",
    "     \n",
    "     - $\\mathcal{e}_r = \\sum_{i = 1}^{n}w_r(i)*(h_r(i) \\neq y_i)$\n",
    "     \n",
    "     - **if** $\\mathcal{e}_r \\gt \\frac{1}{2}$ then stop -- [$\\frac{1}{1 - c}$ where $c$ is the number of\n",
    "unique class labels for multiple classes]\n",
    "\n",
    "     - $\\alpha_r = \\frac{1}{2}log(\\frac{1 - \\mathcal{e}_r}{\\mathcal{e}_r})$ -- [small if error is large and vice versa, for multiple classes use: $\\alpha_r = log(\\frac{1 - \\mathcal{e}_r}{\\mathcal{e}_r}) - log(c - 1)$]\n",
    "     \n",
    "     - $w_{r + 1}(i) = w_r(i) \\times \\begin{cases}\n",
    "                                        e^{-\\alpha_r},& \\text{if } h_r(\\vec{x_i}) = y_i\\\\\n",
    "                                        e^{\\alpha_r},& \\text{if } h_r(\\vec{x_i}) \\neq y_i\n",
    "                                     \\end{cases}\n",
    "       $  \n",
    "  5. Prediction $\\hat{y}_{pred} = \\arg\\!\\max_j \\sum_{r = 1}^{k} \\alpha_r (h_r(\\vec{x}) = j)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16eb9146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1b0d491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.97%\n"
     ]
    }
   ],
   "source": [
    "classification_tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 1, max_depth = 2)\n",
    "classifier_boost = AdaBoostClassifier(base_estimator = classification_tree, n_estimators = 100, algorithm = 'SAMME')\n",
    "\n",
    "classifier_boost.fit(X = X_class_train, y = y_class_train)\n",
    "\n",
    "print(f'Validation Accuracy: {classifier_boost.score(X_class_valid, y_class_valid):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468c192e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "132e4136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation R^2 Score: 0.54%\n"
     ]
    }
   ],
   "source": [
    "regression_tree = DecisionTreeRegressor(random_state = 1, max_depth = 3)\n",
    "regression_boost = AdaBoostRegressor(base_estimator = regression_tree, n_estimators = 100, random_state = 1, learning_rate = .1)\n",
    "\n",
    "regression_boost.fit(X = X_regression_train, y = y_regression_train)\n",
    "\n",
    "print(f'Validation R^2 Score: {regression_boost.score(X_regression_valid, y_regression_valid):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d112ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9686410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c687418b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b9f350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5271fb01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
